# ğŸ§¬ Helical AI Platform â€“ Bio Foundation Models Web Interface

This project is a full-stack web application enabling users to **explore**, **select**, and **run Bio Foundation Models** such as **Geneformer** and **scGPT** for biological use cases like **cell annotation**.

It includes a FastAPI backend, React (Next.js) frontend (with shadcn/ui), and task execution via **Celery + Redis**. Both models are **preloaded in memory** for low-latency inference, and results (e.g. confusion matrices) are persisted in a database.

---

## ğŸš€ Features

- ğŸ”¬ Run **Geneformer** and **scGPT** for **cell type annotation**
- ğŸ§  Load models into RAM at startup
- ğŸ“Š Store UMAP points, probabilities, and metadata per run
- ğŸŒ REST API to query applications and models
- âš™ï¸ Task queue via **Celery + Redis**
- ğŸ³ Dockerized environment for local and cloud deployment

---

## ğŸ—ï¸ Architecture

```bash
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ models.py          # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ database.py        # DB session and setup
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py          # FastAPI endpoints
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â””â”€â”€ tasks.py           # Celery tasks
â”‚   â”œâ”€â”€ load_models.py         # Model loading logic
â”‚   â”œâ”€â”€ celery_worker.py       # Celery entrypoint
â”‚   â””â”€â”€ main.py                # FastAPI app
â”œâ”€â”€ frontend/                  # React frontend (shadcn/ui)
â”œâ”€â”€ docker-compose.yml         # Orchestrates FastAPI, Redis, and Celery
â””â”€â”€ requirements.txt

```

---

## ğŸ§¬ Supported Models

| Model       | Speed | Accuracy | Applications     | Description                     |
|-------------|-------|----------|------------------|---------------------------------|
| Geneformer  | fast  | 0.92     | Cell Annotation  | Transformer model for scRNA-seq |
| scGPT       | slow  | 0.95     | Cell Annotation  | GPT-style model for single-cell |

---

## ğŸ“¡ API Endpoints

| Method | Endpoint                      | Description                             |
|--------|-------------------------------|-----------------------------------------|
| GET    | `/applications`               | List available applications             |
| GET    | `/models`                     | List all models with attributes         |
| GET    | `/applications/{id}/models`   | Get models linked to an application     |
| POST   | `/submit`                        | Submit job (model, application, input)  |
| GET    | `/results/{run_id}`           | Fetch run output      |
| GET    | `/status/{run_id}`           | Fetch run status and stored output      |
| POST    | `/upload`           | Upload data      |
| GET    | `/download/{run_id}`           | Download annotated data      |

---

## âš™ï¸ Asynchronous Task Handling

Uses [**Celery**](https://docs.celeryq.dev) with [**Redis**](https://redis.io/) to handle long-running model inference jobs asynchronously.

---

## ğŸ’¾ Storing Results

Each prediction stores:
- Cell type probabilities
- Raw model outputs
- Metadata

These are stored as structured JSON (e.g. via SQLiteâ€™s `Text` field or PostgreSQL `JSONB`), and as CSV.

---

## ğŸ§  Model Loading

Both models are loaded **at startup** and remain in RAM for fast access. This is suitable for small-to-medium scale deployments.

For large-scale, batch-heavy workloads, we recommend offloading inference to a **SLURM cluster**, **Ray**, or **Kubernetes-based** setup with GPU scheduling.

---

## ğŸ³ Quickstart (Docker)

```bash
# Start API, Celery, Redis
docker-compose up --build
```

Once running:
- FastAPI backend â†’ http://localhost:8000
- Swagger docs â†’ http://localhost:8000/docs
- Front end application â†’ http://localhost:3000

---

## ğŸ§ª Requirements

- Python 3.10+
- FastAPI, SQLAlchemy, Celery, Redis
- PyTorch with GPU (optional but recommended)
- See `requirements.txt`

---

## ğŸ“¬ Contact

Vincent Lefeuve â€“ [vincent.lefeuve24@imperial.ac.uk](mailto:vincent.lefeuve@imperial.ac.uk)


